---
title: "Accuracy and Beyond in Restaurant Recommender Systems"
author: "Christina Valore, Juliann McEachern, & Rajwant Mishra"
date: "July 2, 2019"
output: 
  html_document:
    theme: united
    highlight: tango
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: paged
    code_folding: hide
---
 
```{r dependencies, echo=F,comment=F,message=F,warning=F,prompt=F}
#dependencies
## data processing packages
library(tidyr); library(dplyr); library(RCurl); library(jsonlite); library(plyr)

##formatting packages
library(knitr); library(kableExtra); library(prettydoc); library(default)

##visualization packages
library(ggplot2)

##recommender packages
library(recommenderlab); library(gbm);library(rsample);library(Metrics);library(caret);library(textmineR)
# Used in Diversity calcualtion 
library(lsa)
library(diveRsity)

# global options
## knit sizing
options(max.print="100"); opts_knit$set(width=75) 

## augment chunk output
opts_chunk$set(echo=T,cache=F, tidy=T,comment=F,message=T,warning=T) #change message/warning to F upon completion

## set table style for consistency
default(kable) <- list(format="html")
default(kable_styling)  <- list(bootstrap_options = "hover",full_width=T, font_size=10)
default(scroll_box) <- list(width = "100%")

## working directory
##try(setwd("~/Github/612-group/project-4"))
```


# Overview

In this assignment, we practiced working with accuracy and other recommender system metrics. We compared the performance of serveral algorithms and analyzed the diversity of our recommendations..

## Data Selection

Our data was sourced from Kaggle's [Restaurant Data with Consumer Ratings](https://www.kaggle.com/uciml/restaurant-data-with-consumer-ratings) collection, which contained several datasets pertaining to restaurants and their patrons. The csv files are stored within our repository in the data folder. 

```{r load-data}
# cusine tags
data1<-read.csv("https://raw.githubusercontent.com/jemceach/612-group/master/project-4/data/chefmozcuisine.csv") 
# restaurant name
data2<-read.csv("https://raw.githubusercontent.com/jemceach/612-group/master/project-4/data/geoplaces2.csv") 
# user ratings
data3<-read.csv("https://raw.githubusercontent.com/jemceach/612-group/master/project-4/data/rating_final.csv") 
# user profile
data4<-read.csv("https://raw.githubusercontent.com/jemceach/612-group/master/project-4/data/userprofile.csv") 
```

## Data Tranformations

We cleaned our data using transformations and regular expression unite our user and restaurant data. 

```{r tidy-data}
# restaurant dataframe
## concatenate restaurant tags 
cuisine <- aggregate(Rcuisine ~., data1, toString) 
## merge & transform
restaurant <- data2 %>% select(placeID, name, city,price, alcohol,smoking_area) %>% mutate(name=tolower(gsub("[\u00ef\u00bf\u00bd\'_']", " ", name))) %>% mutate(city=tolower(city)) 
## set/revalue factors 
restaurant$city <- revalue(restaurant$city, c("cd victoria"="ciudad victoria", "cd. victoria"="ciudad victoria","victoria "="ciudad victoria","victoria"="ciudad victoria","san luis potosi "="san luis potosi","san luis potos"="san luis potosi","s.l.p"="san luis potosi","slp"="san luis potosi","s.l.p."="san luis potosi")) 
restaurant$smoking_area = revalue(restaurant$smoking_area, c("none"="no", "not permitted"="no", "section"="yes", "permitted"="yes", "only at bar"="yes")) 
restaurant$price <- factor(restaurant$price, levels = c("low", "medium", "high"))
restaurant$alcohol = revalue(restaurant$alcohol, c("No_Alcohol_Served"="no", "Full_Bar"="yes", "Wine-Beer" ="yes"))
restaurant <- inner_join(restaurant, cuisine, by="placeID")

# user dataframe: 
##select attributes of interest from profile
user_profile <- data4 %>% select(userID, budget,activity, smoker)
user <- inner_join(data3,user_profile,by='userID') %>% select(-service_rating, -food_rating)
## set/revalue factors
user$budget <- factor(user$budget, levels = c("low", "medium", "high"))
user$smoker <- revalue(user$smoker, c("false"="no", "true"="yes")) 
user$smoker <- factor(user$smoker, levels = c("no", "yes"))
## change ratings from 0-2 scale to 1-3
user$rating[user$rating==2]<-3;user$rating[user$rating==1]<-2;user$rating[user$rating==0]<-1

# combine user / restaurant data & subset 
data <- inner_join(user, restaurant, by="placeID") 
data <- data %>% filter(city == "san luis potosi", activity=="student") %>% select(-city, -activity)
```

The output of which can be previewed below: 

```{r data-preview, echo=F}
#view output
data %>% head() %>% kable(caption="User-Item Dataframe") %>% kable_styling()
```

## Data Exploration

We found that 80% of our raters were students students and 76% of our restaurants were located within the Mexican city of San Luis Potosi.  As a result, we subsetted our restaurant/patron data to limit the scope of our system to this specific population. After subsetting our raw data, we identified 78 unique users and 56 restaurants to build our recommender systems from. 

The following plots help visualize the distribution of our overall ratings given by users based on their budget and the restaurant's categorized pricing. We also viewed the rating counts each restaurant received. On average, each venue received 13 user ratings. 

## Data Visualizations {.tabset}

We found that most of our restaurants received high ratings of 3. Most ratings came from users with low-medium budgets and that the majority of our restaurants were in the middle bracket for price. We had a wide spread in the number of ratings each restaurant received, with the fewest being 3 and the most being 32 ratings from our users. There were 32 unique cuisine tags assigned to our restaurants. Mexican cuisine was the most popular and those restaurants received 108 total ratings.

### Raw Ratings

```{r budget-plot, fig.height=2, out.width = '100%', echo=F}
data %>%  group_by(rating) %>% ggplot(aes(x=rating)) + geom_histogram(bins=3, color='#000000', fill='#6ba099') +labs(title="Raw Ratings Distribution by Budget") 
```

### Ratings by Budget

```{r budget-plot, fig.height=2, out.width = '100%', echo=F}
data %>% filter(!is.na(budget)) %>% group_by(rating) %>% ggplot(aes(x=rating)) + geom_histogram(bins=3, color='#000000', fill='#6ba099') +labs(title="Ratings Distribution by Budget") + facet_wrap(~budget, nrow=1)
```

### Ratings by Price

```{r price-plot, fig.height=2, out.width = '100%', echo=F}
ggplot(data, aes(x=rating)) + geom_histogram(bins=3, color='#000000', fill='#6ba099') +labs(title="Ratings Distribution by Restaurant Pricing") + facet_wrap(data$price, nrow=1)
```

### Ratings by Restaurant

```{r restaurant-plot,fig.height=3, out.width = '100%', echo=F}
data %>% mutate(placeID = as.factor(placeID), rating=as.factor(rating)) %>% group_by(placeID) %>% add_tally() %>% ungroup() %>% ggplot(aes(x=reorder(placeID, -n),fill=rating))+ geom_bar(stat="count", color="#000000") + theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank())+labs(title="Restaurant Rating Counts") + scale_fill_manual(values=c("#cccccc", "#44867d", "#00332c"))

data %>%select(placeID) %>% mutate(placeID = as.factor(placeID)) %>% group_by(placeID) %>% count() %>% arrange(freq)
```

### Ratings by Cuisine

```{r cuisine-plot, fig.height=4.5, out.width = '100%', echo=F}
data %>% select(Rcuisine, rating) %>% mutate(rating=as.factor(rating)) %>% group_by(Rcuisine) %>% add_tally() %>% ungroup %>% ggplot(aes(x=reorder(Rcuisine, -n), fill=rating)) + geom_bar(stat="count",color="#000000")+labs(title="Restaurant Cuisine Tag Counts with Ratings") + theme(axis.title.x=element_blank(), axis.text.x = element_text(angle = 30, hjust = 1, size = 8))+ scale_fill_manual(values=c("#cccccc", "#44867d", "#00332c"))
```

## Matrix Building 

Our final transformations converted our raw ratings data into a user-item matrix to test and train our subsequent recommender system algorithms.

```{r build-matrix}
# create user item matrix
ui_matrix <- data %>% select(userID, placeID, rating) %>% spread(placeID, rating)
rownames(ui_matrix)<-ui_matrix$userID # set row names to userid
ui_matrix<-ui_matrix %>% select(-userID) %>% as.matrix()# remove userid from columns
umat <- as(ui_matrix,"realRatingMatrix") # save real ratings for algo 
```

The output of which can be previewed below: 
```{r view-matrix, echo=F}
## preview matrices 
as.data.frame.array(ui_matrix) %>% head() %>% kable(caption="User Matrix Preview (Ratings Preserved as NA)") %>% kable_styling()
```



# Collaborative Filtering 

**ADD TEXT EXPLINATION OF BUSINESS/USER GOALS: Serendipity? Goal to be recommending relevant items to targeted users that are different then items the user has already rated?**



## Training and Test Subsets

Our data was split into training and tests sets for model evaluation of both two recommender algorithms. We split our data using the `recommenderlab` package. 90% of data was retained for training and 10% for testing purposes.

```{r}
# evaluation method using training set of 
evalu <- evaluationScheme(umat, method="split", train=0.9, given=3, goodRating=0)
```


## Algorithm 1: User-Based (Christina)

ADD TEXT ETC. 

### Process 
Text
```{r, echo=T}
#non-normalized
ub_n <- Recommender(getData(evalu, "train"), "UBCF", 
      param=list(normalize = NULL, method="Cosine"))

#centered
ub_c <- Recommender(getData(evalu, "train"), "UBCF", 
      param=list(normalize = "center",method="Cosine"))

#Z-score normalization
ub_z <- Recommender(getData(evalu, "train"), "UBCF", 
      param=list(normalize = "Z-score",method="Cosine"))
```

### Predictions 
Text
```{r}
#predicted ratings
p1 <- predict(ub_n, getData(evalu,"known"),type="ratings")
p2 <- predict(ub_c, getData(evalu,"known"),type="ratings")
p3 <- predict(ub_z, getData(evalu,"known"),type="ratings")

#ceiling and floor values
p1@data@x[p1@data@x[] < 1] <- 1
p1@data@x[p1@data@x[] > 3] <- 3

p2@data@x[p2@data@x[] < 1] <- 1
p2@data@x[p2@data@x[] > 3] <- 3

p3@data@x[p3@data@x[] < 1] <- 1
p3@data@x[p3@data@x[] > 3] <- 3

#compare the predictions using the different normalize methods
error_UCOS <- rbind(
  ub_n = calcPredictionAccuracy(p1, getData(evalu, "unknown")),
  ub_c = calcPredictionAccuracy(p2, getData(evalu, "unknown")),
  ub_z = calcPredictionAccuracy(p3, getData(evalu, "unknown"))
)
error_UCOS %>% kable(caption="Prediction Comparisons") %>% kable_styling()
```

## Algorithm 2 (Raj)
## SVD/FSVD/ALS (Raj)

We will try to use *Singular Value Decomposition* *(SVD)* , Funk SVD *(FSVD)* and *Alternating Least Squares (ALS)*  with latent features starting with 10 and then increasing and decresing it to find better RMSE of our predictor. 

### SVD Data Preparation 
We have three type of data from evaluationScheme: 
+ "train" returns the training data for the run, 
+ "known" returns the known ratings used for prediction for the test data, and 
+ "unknown" returns the ratings used for evaluation for the test data.

```{r echo=FALSE}

print("Glimpse of Data:")
head(data)

# Training Dataset 
ratings_train <- getData(evalu, 'train')
# Test data from evaluationScheme of type KNOWN
ratings_test_known <- getData(evalu, 'known')
# Unknow datset used for RMSE / model evaluation 
ratings_test_unknown <- getData(evalu, 'unknown')


print("Train Data")
ratings_train
print("Test Data:")
ratings_test_known
print("Evaluation Data:")
ratings_test_unknown

```


Below scatterplot shows that our rating is spread heavily among 2 and 3 as the smooth line is above 2. We could opt for multiple options here like taking average of all the three ratings and then using it as baseline for further analysis or we could just use the rating of the Hotel and build our recommendor on it.

After normalization with Z-score out data looks very much centered arround 0 and More towards the top two std Deviation. 


```{r echo=FALSE}

scatter.smooth(getRatings(ratings_train))
scatter.smooth(getRatings(normalize(ratings_train,method="Z-score")))
#  Z-score normalization reduces the variance to a range of roughly
# ???3 to +3 standard deviations. It is interesting to see that there is a pronounced peak of
# ratings between zero and two
hist(getRatings(normalize(ratings_train,method="Z-score")))
```

```{r include=FALSE}
recommenderRegistry$get_entry_names()
recommenderRegistry$get_entries(dataType='realRatingMatrix')
# The description and default parameters of this method in recommenderlab are as follows:
recommenderRegistry$get_entry('SVD', dataType='realRatingMatrix')

```



### Working with Funk Singular Value Decompostion SVD

Funk SVD decomposes a matrix (with missing values) into two components 
U  and V
. The singular values are folded into these matrices. The approximation for the original matrix can be obtained by R = UV'

```{r echo=FALSE}
# library("recosystem")
# library("svd")
# library(Metrics)
# #linear algebra  here used for rmse calculation
# suppressWarnings(suppressMessages(library(Metrics)))
# 
# funkSVD(x, k = 10, gamma = 0.015, lambda = 0.001,
#   min_improvement = 1e-06, min_epochs = 50, max_epochs = 200,
#   verbose = FALSE)
# Buid Model with K =  10 default
fsvd <- funkSVD(ratings_train, verbose = TRUE) # k = 10, gamma = 0.015, lambda = 0.001,


### reconstruct the rating matrix as R = UV'
### and calculate the root mean square error on the known ratings
ratings_train_predict <- tcrossprod(fsvd$U, fsvd$V)
ratings_train_predict  <- as(ratings_train_predict,"realRatingMatrix")



 fsvd10_train <- calcPredictionAccuracy(
  ratings_train,  ratings_train_predict
  )
fsvd10_train

### For Test data

fsvd10_predict <- predict(fsvd, ratings_test_known, verbose = TRUE)
ratings_test_unknown_predict  <- as(fsvd10_predict,"realRatingMatrix")
fsvd10_test = calcPredictionAccuracy(
 ratings_test_unknown_predict,  ratings_test_unknown
  )
fsvd10_test

Result = rbind(
  FSVD_Train_10 = fsvd10_train,
  FSVD_Test_10 = fsvd10_test
)
# Working Code commented to save dispaly space
# print("View the unknon Evalutation matrix")
# as.data.frame.array(as(ratings_test_unknown, "matrix")) %>% head() %>% kable(caption="ratings_test_unknown") %>% kable_styling()%>% scroll_box()
# 
# 
# print("View the Predicted matrix ")
# as.data.frame.array(as(ratings_test_unknown_predict, "matrix")) %>% head() %>% kable(caption="ratings_test_unknown_predict ") %>% kable_styling()%>% scroll_box()

kable(Result,caption=" Result of Funk SVD") 
```


### SVD Method 

We used the SVD algorthim to create a model which makes user-item recommendation predictions. 

We chose to build our SVD model for our recommender system by using 10 default Latent Factor, Latent factors are the features that both User and Item have in common within a given user-item matrix.

With normalize parameter we are asking system to normalize the rating by subtracting avg rating per user. Two availble methods are :

+ *Normalization* tries to reduce the individual rating bias by row centering the data, i.e., by subtracting from each available rating the mean of the ratings of that user (row). 

+ *Z-score* in addition divides by the standard deviation of the row/column. Normalization can also be done on columns.

```{r echo=FALSE}
# Latent-Factor Collaborative Filtering Recommender
# with matrix factorization by Singular-Value Decomposition (SVD)
svdr <- Recommender(
   data=ratings_train,
   method='SVD',            # Recommender based on SVD approximation with column-mean imputation.

   param=list(
     k=10,         # number of latent factors
     normalize='center'    # normalizing by subtracting average rating per user;
                           # note that we don't scale by standard deviations here;
                           # we are assuming people rate on the same scale but have
                           # different biases
    
   ))

 #======================================= Returns NAN 
# Predicton on Train Data
 
 svd10_Train_pred <- predict(svdr,ratings_train,type='ratings')
 
# Evaluation of Train Predication 
 
 svd10_Train_Acc <- calcPredictionAccuracy(
  svd10_Train_pred,
  ratings_train)
 #=======================================
 
 
 # Doing Test on Test data.
 
   svd10_pred <- predict(
   svdr,
   ratings_test_known,
   type='ratings')

  # Evaluate the result from Unknown evluationfeare data.
   
svd10_Test <- calcPredictionAccuracy(
  svd10_pred,
  ratings_test_unknown)

Result <- rbind(Result,
                SVD_Train_10 = svd10_Train_Acc,
                SVD_Test_10 = svd10_Test)

print(paste("the number of latent features (k) is ",getModel(svdr)$k))
# as.data.frame.array(as(ratings_test_unknown, "matrix")) %>% head() %>% kable(caption="ratings_test_unknown_predict ") %>% kable_styling()%>% scroll_box()

# as.data.frame.array(as(svd10_pred, "matrix")) %>% head() %>% kable(caption="svd10_pred ") %>% kable_styling()%>% scroll_box()

kable(Result,caption=" Result of SVD Test ")
```

### Using SVD With Z score
Z-score in addition divides by the standard deviation of the row/column. Normalization can also be done on columns.
```{r echo=FALSE}

# Latent-Factor Collaborative Filtering Recommender
# with matrix factorization by Singular-Value Decomposition (SVD)
svdz<- Recommender(
   data=ratings_train,
   method='SVD',            # Item-Based Collaborative Filtering

   param=list(
     k=10,         # number of latent factors
     normalize='Z-score'   # normalizing by subtracting average rating per user and the devide by std Deviation;
  ))

   svdz10_Pred <- predict(
   svdz,
   ratings_test_known,
   type='ratings')
   
   
svdz10_Test <- calcPredictionAccuracy(
  svdz10_Pred,
  ratings_test_unknown)

Result <- rbind(Result,
                SVD_Z_Test_10 = svdz10_Test)
kable(Result,caption="RMSE With SVD on ZSCORE")

```


##### Finding K for SVD
Finding K for SVD by putting a function to find K and check RMSE
```{r echo=TRUE}


findKforSVD <- function(train,test,evalTest,k=5) {
  #Create the recommender based on SVD and SVDF using the training data
r.svd <- Recommender(train, "SVD", param=list(
     k=k,         # number of latent factors
     normalize='center'
   ))
r.svdz = Recommender(train, "SVD", param=list(
     k=k,         # number of latent factors
     normalize='z-score'
   ))
r.svdf <- Recommender(train, "SVDF")
r.als <- Recommender(train, "ALS")

#Compute predicted ratings for test data that is known using the UBCF algorithm
p.svd <- predict(r.svd, test, type = "ratings")
p.svdf <- predict(r.svdf, test, type = "ratings")
p.als <- predict(r.als,test, type = "ratings")
p.svdz <- predict(r.als,test, type = "ratings")

getRatingMatrix(p.svd)[1:6,1:6]
getRatingMatrix(p.svdf)[1:6,1:6]
getRatingMatrix(p.als)[1:6,1:6]

#Calculate the error between training prediction and unknown test data

error <- rbind(
  SVD = calcPredictionAccuracy(p.svd, evalTest),
  SVDZ = calcPredictionAccuracy(p.svdz, evalTest),
  SVDF = calcPredictionAccuracy(p.svdf, evalTest),
  ALS = calcPredictionAccuracy(p.als,evalTest))
kable(as.data.frame(error))
  return(error)
}

error = rbind(
  findKforSVD(getData(evalu, "train"), getData(evalu, "known"),getData(evalu, "unknown"),5),
findKforSVD(getData(evalu, "train"), getData(evalu, "known"),getData(evalu, "unknown"),6),
findKforSVD(getData(evalu, "train"), getData(evalu, "known"),getData(evalu, "unknown"),10))
  
kable(error,caption = "For K 5 6 10 in group of 4")


```

### Diversity

Diversity measures how dissimilar recommended items are for a user. This similarity is often determined using the item's content (e.g. Hotel proposed) but can also be determined using how similarly items are rated. One measure of diversity is the Intra-List Similarity (ILS). The ILS equation can calculate the similarity between any two items (ij, ik) using the cosine similarity, Jaccard similarity coefficient, or another similarity metric could be utilized in the equation.

In our Diversity test, we are using predicted rating of the Hotel from the USER and Actual rating of the hotel from the User. Ten we only use the items that have been rate from the original dataset and calculate the cosine similarity among the predicted and actual rating.  


```{r echo=TRUE}
# Funcation to calualte Diversity

getDivesity <- function(ActaulData,Predicated,tag) {
cal_cosine <- NULL  
# Printing the MAtrix 
print(as.data.frame.array(as(Predicated,  "matrix")) %>% head() %>% kable(caption="Predicated Rating  ") %>% kable_styling()%>% scroll_box())

print(as.data.frame.array(as(ActaulData,  "matrix")) %>% head() %>% kable(caption="Actaul Rating ") %>% kable_styling()%>% scroll_box())
  
# COnverting the result realmatrix to matrix 
Predicated= as(Predicated,  "matrix")
ActaulData= as(ActaulData,  "matrix")

  for (i in 1:(dim(ActaulData)[1])) {
    a <- as.vector(ActaulData[i,])
    b <- as.vector(Predicated[i, ])
    # set All NA from MAIN Data = 0
    b[which(is.na(a))]=0
    a[which(is.na(a))]=0
      
    cal_cosine[i] <- cosine(a ,b)  #a %*% b / sqrt(a%*%a * b%*%b)
}



print("============Caculating Diversity using cosine similarity==================")
print (cal_cosine)

print("==============================Valdiate With Data==========================")
print("==========================================================================")


a <- as.vector(ActaulData[2,])
b <- as.vector(Predicated[2, ])
b[which(is.na(a))]=0
a[which(is.na(a))]=0
print(paste("Actual Rating:=> ",a[which(a != 0)], "| Predicted Rating:=> ",round(b[which(b != 0)],2)))

print(paste("Cosine for 2nd index of Test data:=> ",
            round(cosine(a[which(a != 0)],b[which(b != 0)]),2)))


print(paste("Cosine for 2nd index of Calcualted Data:=> ",round(cal_cosine[2],2)))

print(paste("Average Diversity noted :=>" , round(mean(cal_cosine),2)))

actaulDIversity_Error = 1 - mean(cal_cosine)

print(paste("Diversity Error [ ", tag ," ]:=> ",round(actaulDIversity_Error,4) ))


}

```

```{r echo=TRUE}
# Diversity with SVD with k = 10
getDivesity(ratings_test_unknown,svd10_pred, "SVD k-10")

# Diversity with SVD with k = 10 and Normalise user rating using Z score 
getDivesity(ratings_test_unknown,svdz10_Pred,"SVD with Z-score")

# Diversity with FSVD with k = 10 and Normalise user rating using Z score 
getDivesity(ratings_test_unknown,fsvd10_predict,"Funk SVD")

```

## Compare 

We noted that SVD with Z score is doing good recommendation in terms of RMSE as well as with Diversity score.<br>
*Diversity Check with SVD of k = 10 * <br>
<b>Diversity Error [  SVD with Z-score  ]:=>  0.028 </b><br>
<b>Diversity Error [  Funck SVD k= 10 ]:=>  0.0301 </b><br>
<b>Diversity Error [  SVD k-10  ]:=>  0.0312</b><br>
<br>
*RMSE Check with SVD of k = 10 * <br>
<b>RMSE  [  SVD with Z-score  ]:=>  0.94</b><br>
<b>RMSE  [  Funck SVD k= 10 ]:=>  0.65</b><br>
<b>RMSE  [  SVD k-10  ]:=>  0.68</b><br>

RMSE was 0.94 with SVD Zscore normalisation and Diversity Error was 0.028 with SVD of Z-score normalisation. FunkSVD seemed doing consistent in all the value of K (5,6,10), but didn't imporve much as comapored with SVD With Z-core on k=10 i.e. 10 latent factors.



#### -------------END OF RAJ WORK

# Hybrid Recommender System (Juliann)

Our data source was rich with categorical features containing user data such as height, weight, religion, personality-types and more.  We wanted to also try to incorporate a hybrid recommender to look at user and restaurant attributers to build our prediction model using a different machine learning techinique -- Gradient Boosting. 

Reference later for [hybrid model algorithms](https://medium.com/@alfonsollanes/how-do-you-measure-and-evaluate-the-quality-of-recommendation-engines-2e91db5952af). 


## Gradient Boosted Model (GBM)

The gradient boosted technique evaluates our data from decision trees. Using an iterative process, the algoritm improves the fit of our inputted variables when creating our predictive model.  We used the `gbm`, `resample`, `Metrics` & `caret` packages to build and evaluate the model.

### Build GBM Model 

Our GBM model measures the relative influence of our select restaurant and  user characteristics to make user-rating predictions. We found that type of cuisine from our tags had significant importance on our model.

```{r}
# Build Model

## subset data for model
PatronDataModel <- data %>% select(userID, placeID,rating, price,alcohol,smoking_area, Rcuisine)

## factor data
PatronDataModel$Rcuisine<-as.factor(PatronDataModel$Rcuisine)

## train test spit
PatronSplit <- initial_split(PatronDataModel, prop = .9)
PatronTrain <- training(PatronSplit)
PatronTest  <- testing(PatronSplit)

## fit model
patron_fit_1 <- gbm::gbm(rating ~., 
             data = PatronDataModel, 
             distribution = "gaussian", #gaussian algo (squared error)
             verbose = F, #change to true to view output
             shrinkage = 0.01, # step size reduction
             interaction.depth = 3, # tree depth for two way interactions
             n.minobsinnode = 5, # min observations
             n.trees = 500, #number of trees to fit/iterate
             cv.folds = 10 #cv folds
             )

show.gbm(patron_fit_1)

## summarize model
summary.gbm(patron_fit_1, plotit = F) %>% kable() %>%kable_styling()

# Predictions

## optimal number of boosting iterations
perf_gbm1 = gbm.perf(patron_fit_1, method = "cv", plot.it = F)

## predict
patron_prediction_1 <- stats::predict(
                          object = patron_fit_1, # fit prediction
                          newdata = PatronTest, # test data 
                          n.trees = perf_gbm1) # optimal n of iterations
```


### Evaluation

We evaluated GBM model using standard RMSE, MSE, MEA, and RMSLE calculations. We also used a confusion matrix to calculate the accuracy of our predictions. 

```{r}
# Metrics
fit_rmse <- Metrics::rmse(actual = PatronTest$rating, predicted =patron_prediction_1)
fit_mse <- Metrics::mse(actual = PatronTest$rating, predicted =patron_prediction_1)
fit_mae <- Metrics::mae(actual = PatronTest$rating, predicted =patron_prediction_1)
fit_rmsle <- Metrics::rmsle(actual = PatronTest$rating, predicted =patron_prediction_1)

#set factors
PatronTest$rating<-as.factor(PatronTest$rating)
fit_predict <- as.factor(round(patron_prediction_1))

#confusion matrix
eval_cm <- confusionMatrix(fit_predict, PatronTest$rating)
fit_accuracy<-round(eval_cm$overall[1],4)

# view metrics
results <- cbind(fit_rmse, fit_mse,fit_mae,fit_rmsle,fit_accuracy)
rownames(results) <- c()
results %>% kable(caption="Model Evaluation Metrics") %>% kable_styling()

### Note: Output shows NA (not meaningful for factors) if chunk is rerun without clearing global environment
```

The confusion matrix from the `caret` package also allowed us to look at the accuracy and percision by each of our predicted rating as a unique class.  

```{r}
eval_cm$byClass %>% round(4) %>% kable(caption = "Confusion Matrix Evaluation by Rating") %>% kable_styling()
```

### Analysis 

Pros:

*  Novelty: Relevant recommendations not based on popularity.

Cons:

*  Over-specialisation
*  May fail diversity test 

Add more later :)



# Conclusion

**As part of your textual conclusion, discuss one or more additional experiments that could be performed and/or metrics that could be evaluated only if online evaluation was possible. Also, briefly propose how you would design a reasonable online evaluation environment.**


Also: Compare content algorith outcomes to hybrid approach. 

# References:


[*Recommender-systems-its-not-all*])(https://gab41.lab41.org/recommender-systems-its-not-all-about-the-accuracy-562c7dceeaff)
[*Cosine Similarity*](https://www.rdocumentation.org/packages/lsa/versions/0.73.1/topics/cosine)
[*EvaluationScheme from Recommenderlab*](https://rdrr.io/cran/recommenderlab/man/evaluationScheme-class.html)
[**Building Recommenders**:](https://buildingrecommenders.wordpress.com/2015/11/19/overview-of-recommender-algorithms-part-3/) Overview of Recommender Algorithms
[**recommenderlab**:](https://cran.r-project.org/web/packages/recommenderlab/vignettes/recommenderlab.pdf) A Framework for Developing and Testing Recommendation Algorithms 

[**UC Business Analytics R Programming Guide**:](http://uc-r.github.io/gbm_regression) Gradient Boosting Machines
